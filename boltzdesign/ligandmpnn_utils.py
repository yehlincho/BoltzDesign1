import os
import sys

import torch
import torch.nn as nn
import torch.optim as optim
from pytorch_lightning import LightningModule

import pickle
import urllib.request
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Literal, Optional
import click
import torch
import json
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.strategies import DDPStrategy
from pytorch_lightning.utilities import rank_zero_only
from tqdm import tqdm

from boltz.data import const
from boltz.data.module.inference import BoltzInferenceDataModule
from boltz.data.msa.mmseqs2 import run_mmseqs2
from boltz.data.parse.a3m import parse_a3m
from boltz.data.parse.csv import parse_csv
from boltz.data.parse.fasta import parse_fasta
from boltz.data.parse.yaml import parse_yaml
from boltz.data.types import MSA, Manifest, Record, Connection, Input, Structure
from boltz.data.write.writer import BoltzWriter
from boltz.model.model import Boltz1
from boltz.main import BoltzProcessedInput, BoltzDiffusionParams
from rdkit import Chem
from boltz.data.feature import featurizer
from boltz.data.tokenize.boltz import BoltzTokenizer, TokenData
from boltz.data.feature.featurizer import BoltzFeaturizer
from boltz.data.parse.schema import parse_boltz_schema
from boltzdesign_utils import predict

from matplotlib.animation import FuncAnimation
import numpy as np
import yaml
from pathlib import Path
import pandas as pd

# Get the project root directory (parent of boltzdesign)
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Add project root and LigandMPNN to path if not already there
if project_root not in sys.path:
    sys.path.append(project_root)
if os.path.join(project_root, 'LigandMPNN') not in sys.path:
    sys.path.append(os.path.join(project_root, 'LigandMPNN'))

from prody import parsePDB
import numpy as np

import yaml
from pathlib import Path
from run import main
from argparse import Namespace
from types import SimpleNamespace


import sys
import logging
from Bio.PDB.MMCIFParser import MMCIFParser
from Bio.PDB import PDBIO

chain_to_number = {
    'A': 0,
    'B': 1,
    'C': 2,
    'D': 3,
    'E': 4,
    'F': 5,
    'G': 6,
    'H': 7,
    'I': 8,
    'J': 9,
}

def int_to_chain(i, base=62):
    """
    Converts a positive integer to a PDB-valid chain ID using uppercase letters,
    numbers, and optionally lowercase letters (base up to 62).
    """
    if i < 0:
        raise ValueError("positive integers only")
    if base < 0 or 62 < base:
        raise ValueError("Invalid base")

    quot = int(i) // base
    rem = i % base
    if rem < 26:
        letter = chr(ord("A") + rem)
    elif rem < 36:
        letter = str(rem - 26)
    else:
        letter = chr(ord("a") + rem - 36)

    if quot == 0:
        return letter
    else:
        return int_to_chain(quot - 1, base) + letter

class OutOfChainsError(Exception):
    pass

def rename_chains(structure):
    """
    Renames chains to be one-letter valid PDB chains.

    Existing one-letter chains are kept. Others are renamed uniquely.
    Raises OutOfChainsError if more than 62 chains are present.
    Returns a map between new and old chain IDs.
    """
    next_chain = 0
    chainmap = {c.id: c.id for c in structure.get_chains() if len(c.id) == 1}
    
    for o in structure.get_chains():
        if len(o.id) != 1:
            if o.id[0] not in chainmap:
                chainmap[o.id[0]] = o.id
                o.id = o.id[0]
            else:
                c = int_to_chain(next_chain)
                while c in chainmap:
                    next_chain += 1
                    if next_chain >= 62:
                        raise OutOfChainsError()
                    c = int_to_chain(next_chain)
                chainmap[c] = o.id
                o.id = c
    return chainmap

def sanitize_residue_names(structure):
    """
    Truncates all residue names to 3 characters (PDB format limit).
    Logs a warning if truncation occurs.
    """
    for model in structure:
        for chain in model:
            for residue in chain:
                resname = residue.resname
                if len(resname) > 3:
                    truncated = resname[:3]
                    logging.warning(f"Truncating residue name '{resname}' to '{truncated}'")
                    residue.resname = truncated

def convert_cif_to_pdb(ciffile, pdbfile):
    """
    Convert a CIF file to PDB format, handling chain renaming and residue name truncation.

    Args:
        ciffile (str): Path to input CIF file
        pdbfile (str): Path to output PDB file

    Returns:
        bool: True if conversion succeeds, False otherwise
    """
    logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.WARNING)

    strucid = ciffile[:4] if len(ciffile) > 4 else "1xxx"

    # Parse CIF file
    parser = MMCIFParser(QUIET=True)
    structure = parser.get_structure(strucid, ciffile)

    # Rename chains
    try:
        rename_chains(structure)
    except OutOfChainsError:
        logging.error("Too many chains to represent in PDB format")
        return False

    # Truncate long ligand or residue names
    sanitize_residue_names(structure)

    # Write to PDB
    io = PDBIO()
    io.set_structure(structure)
    io.save(pdbfile)
    return True

def convert_cif_files_to_pdb(results_dir, save_dir, af_dir=False, high_iptm=False, i_ptm_cutoff = 0.5):
    """
    Convert all .cif files in results_dir to .pdb format and save in save_dir
    
    Args:
        results_dir (str): Directory containing .cif files
        save_dir (str): Directory to save converted .pdb files
        af_dir (bool): If True, look for _model.cif_model.cif files instead of .cif
    """
    confidence_scores = []
    os.makedirs(save_dir, exist_ok=True)
    for root, dirs, files in os.walk(results_dir):
        for file in files:
            if af_dir:
                if file.endswith('_model.cif'):
                    cif_path = os.path.join(root, file)
                    pdb_path = os.path.join(save_dir, file.replace('.cif', '.pdb'))
                    print(f"Converting {cif_path}")
                    
                    if high_iptm:
                        confidence_file_summary = [os.path.join(root, f) for f in os.listdir(root) if f.endswith('summary_confidences.json')][0]
                        confidence_file = [os.path.join(root, f) for f in os.listdir(root) if f.endswith('_confidences.json') and not f.endswith('summary_confidences.json')][0]
                        with open(confidence_file_summary) as f:
                            confidence_data = json.load(f)
                            iptm = confidence_data['iptm']

                        with open(confidence_file, 'r') as f:
                            confidence_data = json.load(f)
                            plddt = np.mean(confidence_data['atom_plddts'])
                            
                        if iptm > i_ptm_cutoff:
                            print(f"Converting {cif_path}") 
                            print(f"iptm score: {iptm}")
                            print(f"pdb_path: {pdb_path}")
                            convert_cif_to_pdb(cif_path, pdb_path)
                            confidence_scores.append({'file': file, 'iptm': iptm, 'plddt': plddt})

                    else:  
                        print(f"Converting {cif_path}")
                        convert_cif_to_pdb(cif_path, pdb_path)
            else:
                if file.endswith('.cif'):
                    cif_path = os.path.join(root, file)
                    pdb_path = os.path.join(save_dir, file.replace('.cif', '.pdb'))
                    print(f"Converting {cif_path}")

                    if high_iptm:
                        confidence_files = [f for f in os.listdir(root) if f.startswith('confidence_') and f.endswith('.json')]
                        if confidence_files:
                            confidence_file = os.path.join(root, confidence_files[0])
                            with open(confidence_file) as f:
                                confidence_data = json.load(f)
                                iptm = confidence_data['iptm']
                            if iptm > i_ptm_cutoff:
                                print(f"Converting {cif_path}")
                                print(f"Confidence file: {confidence_file}")
                                print(f"iptm score: {iptm}")
                                convert_cif_to_pdb(cif_path, pdb_path)
                        
                    else:
                        print(f"Converting {cif_path}")
                        convert_cif_to_pdb(cif_path, pdb_path)
                        
            if confidence_scores:
                confidence_scores_path = os.path.join(save_dir, 'high_iptm_confidence_scores.csv')
                pd.DataFrame(confidence_scores).to_csv(confidence_scores_path, index=False)
                print(f"Saved confidence scores to {confidence_scores_path}")


def get_protein_ligand_interface(pdb_id, cutoff=6, non_protein_target=True, binder_chain='A', target_chain='B'):
    """
    Get interface residues between a protein and ligand/target from a PDB structure.
    
    Args:
        pdb_id (str): Path to PDB file
        cutoff (float): Distance cutoff in Angstroms for interface residues
        non_protein_target (bool): Whether to select non-protein ligand (True) or protein target chain (False)
        binder_chain (str): Chain ID of the protein binder chain
        target_chain (str): Chain ID of the target protein chain (if non_protein_target=False)
    
    Returns:
        list: Residue indices that are at the interface
    """
    # Load PDB structure
    pdb = parsePDB(pdb_id)
    
    # Get binder protein coordinates
    protein = pdb.select(f'protein and chain {binder_chain} and name CA')
    protein_coords = protein.getCoords()
    
    # Get ligand/target coordinates
    if non_protein_target:
        ligand = pdb.select('not protein and not water')
    else:
        ligand = pdb.select(f'protein and chain {target_chain} and name CA')
        
    ligand_coords = ligand.getCoords()
    
    # Calculate pairwise distances and find interface residues
    distances = np.sqrt(np.sum((protein_coords[:,None,:] - ligand_coords[None,:,:])**2, axis=-1))
    interacting = distances < cutoff
    interface_residues = list(np.nonzero(np.sum(interacting, axis=-1))[0])
    
    return interface_residues

def get_protein_ligand_interface_all_atom(pdb_id, cutoff=6, non_protein_target=True, binder_chain='A', target_chains=None):
    """
    Get interface residues between a protein and ligand/target from a PDB structure.
    Args:
        pdb_id (str): Path to PDB file
        cutoff (float): Distance cutoff in Angstroms for interface residues
        non_protein_target (bool): Whether to select non-protein ligand (True) or protein target chain (False)
        binder_chain (str): Chain ID of the protein binder chain
        target_chains (list): Chain IDs of the target protein chains (if non_protein_target=False). If None, all chains except binder_chain are used.
    
    Returns:
        list: Residue indices that are at the interface
    """
    # Load PDB structure
    pdb = parsePDB(pdb_id)
    
    # Get binder protein coordinates
    protein = pdb.select(f'protein and chain {binder_chain}')
    protein_residues = set(protein.getResnums())
    

    if non_protein_target:
        ligand = pdb.select('not protein and not water')
        ligand_atoms = ligand.getCoords()
        protein_atoms = protein.getCoords()
        
        distances = np.sqrt(np.sum((protein_atoms[:,None,:] - ligand_atoms[None,:,:])**2, axis=-1))
        interacting = distances < cutoff
        
        interface_residues = set()
        for i in range(len(protein)):
            if np.any(interacting[i]):
                interface_residues.add(protein.getResnums()[i])
                
    else:
        if target_chains == None:
            all_chains = set([chain.getChid() for chain in pdb.select('protein').getHierView().iterChains()])
            target_chains = list(all_chains - {binder_chain})
            print("target_chains", target_chains)
            
        interface_residues = set()
        for target_chain in target_chains:
            ligand = pdb.select(f'protein and chain {target_chain}')
            ligand_atoms = ligand.getCoords()
            protein_atoms = protein.getCoords()
            
            # Calculate all atom distances
            distances = np.sqrt(np.sum((protein_atoms[:,None,:] - ligand_atoms[None,:,:])**2, axis=-1))
            interacting = distances < cutoff
            
            # Map interacting atoms back to residues
            for i in range(len(protein)):
                if np.any(interacting[i]):
                    interface_residues.add(protein.getResnums()[i])
    
    sorted_residues = sorted(list(protein_residues))
    interface_indices = [sorted_residues.index(res) for res in interface_residues]
    
    return sorted(interface_indices)

def run_ligandmpnn_redesign(
    base_dir,
    pdb_dir,
    ccd_path,
    boltz_model,
    yaml_dir,
    ligandmpnn_config,
    top_k=5,
    cutoff=6,
    non_protein_target=True,
    binder_chain='A',
    target_chains=['B'],
    fix_interface=True,
    out_dir=None,
    lmpnn_yaml_dir=None,
    results_final_dir=None
):
    # Set default output directories if not provided
    if out_dir is None:
        out_dir = os.path.join(base_dir, 'lmpnn_redesigned_fa')
    if lmpnn_yaml_dir is None:
        lmpnn_yaml_dir = os.path.join(base_dir, 'lmpnn_redesigned_yaml')
    if results_final_dir is None:
        results_final_dir = os.path.join(base_dir, 'lmpnn_redesigned')

    # Create required directories
    for directory in [out_dir, lmpnn_yaml_dir, results_final_dir]:
        os.makedirs(directory, exist_ok=True)

    # Initialize score tracking lists
    original_score = []
    ligandpmpnn_redesign_score = []

    for pdb_path in os.listdir(pdb_dir):
        pdb_name = pdb_path.split('.pdb')[0]
        existing_yamls = list(Path(lmpnn_yaml_dir).glob(f'{pdb_name}_*.yaml'))
        if existing_yamls:
            print(f"Skipping {pdb_name} as yaml files already exist")
            continue
        else:
            pdb_path = os.path.join(pdb_dir, pdb_path)
            if pdb_path.endswith('.pdb'):
                interface_residues= get_protein_ligand_interface_all_atom(pdb_path, cutoff=cutoff, non_protein_target=non_protein_target, binder_chain=binder_chain, target_chains=target_chains)
                print("len interface_residues", len(interface_residues))
                with open(ligandmpnn_config, 'r') as f:
                    config_dict = yaml.safe_load(f)

                if non_protein_target:
                    model_type = "ligand_mpnn"
                else:
                    model_type = "soluble_mpnn"

                config = SimpleNamespace(**config_dict)
                config.model_type = model_type
                config.seed = 111
                config.pdb_path = pdb_path
                config.out_folder = out_dir
                if fix_interface:
                    config.fixed_residues = " ".join([f'{binder_chain}{item+1}' for item in interface_residues])
                config.batch_size = 16
                config.save_stats = 0
                config.chains_to_design = binder_chain
                output = main(config)
                fasta_path = os.path.join(out_dir, 'seqs', f'{pdb_name}.fa')
                print(fasta_path)
                # Read the existing fa file
                with open(fasta_path, 'r') as f:
                    lines = f.readlines()
                    sequences = []
                    sequence_found = False  # Flag to check if sequence is found
                    for line in lines[2:]:
                        if line.startswith('>'):
                            overall_confidence = float(line.split(',')[4].split('=')[1])
                            ligand_confidence = line.split(',')[5].split('=')[1]
                            sequences.append((overall_confidence, ligand_confidence, ""))  # Store confidence and ligand
                            sequence_found = True  # Set flag to true if sequence is found
                        elif sequence_found:  # Check for the sequence line after finding confidence
                            sequences[-1] = (sequences[-1][0], sequences[-1][1], line.strip())  # Add the corresponding sequence
                            sequence_found = False  # Reset flag after capturing the sequence
                top_sequences = sorted(sequences, key=lambda x: x[0], reverse=True)[:top_k]
                for idx, (overall_confidence, ligand_confidence, sequence) in enumerate(top_sequences):
                    matching_yamls = list(Path(yaml_dir).glob(f'{pdb_name.split("_results")[0]}*.yaml'))
                    if matching_yamls:
                        yaml_path = str(matching_yamls[0])  # Take the first matching yaml file
                        with open(yaml_path, 'r') as f:
                            yaml_data = yaml.safe_load(f)
                    
                    # Remove constraints
                    yaml_data.pop('constraints', None)
                    
                    if not non_protein_target:
                        binder_idx = chain_to_number[binder_chain]
                        yaml_data['sequences'][binder_idx]['protein']['sequence'] = sequence.split(':')[chain_to_number[binder_chain]]
                    else:
                        yaml_data['sequences'][chain_to_number[binder_chain]]['protein']['sequence'] = sequence

                    # Replace .npz with .a3m in msa paths
                    for seq in yaml_data['sequences']:
                        if 'protein' in seq and 'msa' in seq['protein']:
                            msa_path = seq['protein']['msa']
                            if isinstance(msa_path, str) and msa_path.endswith('.npz'):
                                seq['protein']['msa'] = msa_path.replace('.npz', '.a3m')

                    final_yaml_path = os.path.join(lmpnn_yaml_dir, f'{pdb_name}_{idx+1}.yaml')
                    with open(final_yaml_path, 'w') as f:
                        yaml.dump(yaml_data, f)

                    predict(
                        data=str(final_yaml_path),
                        ccd_path=Path(ccd_path),
                        out_dir=str(results_final_dir),
                        model_module=boltz_model,
                        accelerator="gpu",
                        num_workers = 1
                    )
                    print(f"Completed processing {pdb_name} for sequence {idx+1}")


